# **RootSpec Framework**

**Version:** 4.6.2
**Last Updated:** 2026-02-18
**Status:** Stable

---

**üìã How to Use This File:**

This is the **RootSpec framework definition** ‚Äî the single seed file you need.

**Installation Options:**

- **With npm/CLI** (recommended): `npx rootspec init`
- **Manual**: Download this file to your project directory

The CLI automatically sets up the framework and generates context-aware prompts for creating your specification. AI assistants will read the examples and guidance in this document to generate your actual specification files (01-05).

**For Humans:** Read this document to understand the framework structure, examples, and guidelines. Use it as a reference when creating or validating specifications.

**For AI Assistants:** This document contains the complete framework specification with examples. Use it to understand the five-level hierarchy, reference rules, YAML formats, and validation checklists. Section markers (HTML comments) help you navigate to specific content.

**RootSpec Repository** (this repo) contains:
- `00.SPEC_FRAMEWORK.md` ‚Üê This file (copy to your project)
- `README.md`, `CLAUDE.md`, `CHANGELOG.md` (documentation)

**Your Project** (after using this framework) will contain:
- `00.SPEC_FRAMEWORK.md` (copied reference)
- `01.FOUNDATIONAL_PHILOSOPHY.md` (your Level 1 spec, generated by AI)
- `02.STABLE_TRUTHS.md` (your Level 2 spec)
- `03.INTERACTION_ARCHITECTURE.md` (your Level 3 spec)
- `04.SYSTEMS/` (your Level 4 specs)
- `05.IMPLEMENTATION/` (your Level 5 specs)

---

## Overview

This document defines a hierarchical specification system enforcing **dependency inversion**: foundational philosophy guides implementation, never vice versa. High-level policy sets direction; low-level details cascade downward.

**Core principle:** Each concern lives at exactly one level (single source of truth). Changes flow downward through abstraction layers while foundational documents remain stable.

**Emergent interaction:** Most software behavior is emergent‚Äîarising from the composition of features, shared UX patterns, and developer judgment. Specifications define intentions, constraints, and core flows, not every possible interaction or microstate. Product managers and developers rely on a shared mental model of expected UX to fill in the gaps. User stories capture meaningful user value and critical paths; routine behaviors are left to standard patterns, lower-level tests, and team intuition.

> **See [CHANGELOG.md](CHANGELOG.md) for version history and migration guides.**

## The Seed Metaphor

Think of this framework file as a seed. It contains the genetic information for your entire specification. You plant it in your project, grow it through AI conversation, and harvest a validated spec.

**The growth process:**
1. **Plant** ‚Äî Copy `00.SPEC_FRAMEWORK.md` to your project
2. **Germinate** ‚Äî AI reads the seed, asks questions, extracts your latent vision
3. **Grow** ‚Äî Develop each level (L1‚ÜíL5) through structured dialogue
4. **Strengthen** ‚Äî Validate with tests; the spec becomes robust through use

**The tree structure:**

| Level | Metaphor | Role |
|-------|----------|------|
| L1: Philosophy | **Roots** | Invisible foundation drawing from first principles |
| L2: Stable Truths | **Trunk** | Structural support channeling philosophy upward |
| L3: Interaction | **Branches** | Patterns that shape how the system manifests |
| L4: Systems | **Leaves** | Visible implementation where design becomes action |
| L5: Implementation | **Fruit** | Observable outcomes users actually experience |

This reinforces dependency inversion: roots feed trunk, trunk feeds branches, branches feed leaves, leaves produce fruit. Nothing flows backward. A healthy tree has strong roots ‚Äî philosophy supports everything above it.

## The Five-Level Hierarchy

### Quick Reference

| Level | Icon | Purpose | Key Question | Audience | References |
|-------|------|---------|--------------|----------|------------|
| **1: Foundational Philosophy** | üìò | WHY & WHAT EXPERIENCE | "What problem must we solve? What should users feel?" | Founders, designers, domain experts | External only |
| **2: Stable Truths** | ‚öôÔ∏è | Design strategies & commitments | "What approach will we take?" | System designers, product strategists | L1 + External |
| **3: Interaction Architecture** | üîÑ | HOW users and product interact | "What's the behavioral pattern?" | Interaction designers, developers | L1-2 + External |
| **4: Systems** | üß© | Implementation architecture | "How do we build this?" | Developers, AI systems, implementers | L1-3 + Sibling L4 + External |
| **5: Implementation** | ‚öñÔ∏è | Validation & numeric tuning | "Does it work? What values?" | PMs, QA, data scientists | All levels + External |

### Level Details

#### üìò **Level 1: Foundational Philosophy** ‚Äî WHY & WHAT EXPERIENCE

**Purpose:** Philosophical, psychological, and experiential foundations that should _never_ change. Defines both why the product exists and what core experiences it creates.

**Key Contents:**
- **(Optional) Competitors** ‚Äî Similar products in the market
- **(Optional) Table Stakes** ‚Äî Core features users expect in this category
- **(Optional) Pain Points** ‚Äî Problems with existing solutions that this solves
- **(Optional) Influences** ‚Äî Products or vibes from other media that inspire
- Mission statement ("Why this product must exist")
- **Design Pillars** (3-5 core experiences/emotions that define the product)
- Inviolable principles
- North-star experience

**Market Context Sections (Optional but Encouraged):**

These four optional sections provide strategic context about the market landscape. They appear before Mission/Design Pillars to inform philosophical foundations:

- **Competitors:** List similar products in the market. Lightweight, just names and brief descriptions.
- **Table Stakes:** Core features users expect in this category. What must exist for the product to be viable.
- **Pain Points:** Problems with existing solutions that this product solves. What competitors get wrong.
- **Influences:** Products or vibes from other media that inspire the approach. What patterns you're drawing from.

**Living Documents:** Update these as the market evolves‚Äînew competitors, shifting expectations, discovered pain points.

**Design Pillars:** Short, emotional phrases (1 sentence max) that capture the essence of the user experience. These serve as decision filters‚Äîfeatures must support at least one pillar. Focus on how users will FEEL, not what they will do.

**Reference Rules:** External resources only. Cannot reference any project documents. Use placeholders for numbers.

**Example:**
```markdown
## Competitors

- **Competitor A** ‚Äî Traditional task manager with complex feature set
- **Competitor B** ‚Äî Minimalist todo app focused on simplicity
- **Competitor C** ‚Äî Productivity suite with calendar integration

## Table Stakes

- Task creation and completion
- Basic organization (lists, tags, or folders)
- Mobile and web access
- Data persistence and sync

## Pain Points

- **Guilt-driven mechanics** ‚Äî Most productivity apps create anxiety about incomplete tasks
- **One-size-fits-all timing** ‚Äî Apps don't account for natural energy rhythms
- **Feature bloat** ‚Äî Core functionality buried under advanced features
- **Rigid workflows** ‚Äî Forced methodologies (GTD, Pomodoro) that don't fit all users

## Influences

- **Habit tracking apps** ‚Äî Visual progress without punishment for breaks
- **Game design** ‚Äî Intrinsic motivation through meaningful choices
- **Chronobiology research** ‚Äî Respecting natural energy cycles
- **Mindfulness practices** ‚Äî Focus on present actions over future anxiety

## Mission
Transform how people interact with [domain] by prioritizing [core value] over [traditional approach].

## Design Pillars

### 1. Empowered Action
Users feel in control and capable, never overwhelmed or helpless.

**User perspective:** "I feel confident I can accomplish what I set out to do."

**Examples:**
- Clear pathways to goals with visible progress
- Gentle guidance without forced behavior

### 2. Sustainable Engagement
Users experience energizing focus, not draining obligation.

**User perspective:** "I feel refreshed and motivated, not burned out."

**Examples:**
- Rewards for effort without punishment for breaks
- Natural stopping points that respect user time

## Inviolable Principles
- Never sacrifice [value] for [competing concern]
- Always prioritize [key constraint]
- No guilt-inducing mechanics (supports Sustainable Engagement pillar)
```

---

#### ‚öôÔ∏è **Level 2: Stable Truths** ‚Äî WHAT Strategy

**Purpose:** Philosophically committed choices about how the product works.

**Key Contents:**
- Design philosophies
- Architectural commitments and patterns
- Definition of "success"
- Constraints and trade-offs

**Reference Rules:** Can reference Level 1 + external. Cannot reference L3-5. Use placeholders for numbers.

**Example:**
```markdown
### Truth: Complexity must be emergent, not imposed
The system should have simple rules that combine to create rich behavior, rather than complex rules that create rigid experiences.
```

---

#### üîÑ **Level 3: Interaction Architecture** ‚Äî HOW (Conceptual)

**Purpose:** Fundamental patterns of user-product interaction before implementation.

**Key Contents:**
- Core interaction loops and patterns
- User behavior models
- Multi-scale architecture (immediate, session, extended, lifetime)
- Edge cases and failure modes
- Verification criteria

**Reference Rules:** Can reference L1-2 + external. Cannot reference L4-5. Use placeholders for numbers.

**Example:**
```markdown
## Interaction Loop Architecture
| Scale     | Scope                | Duration   | Key Systems         |
|-----------|----------------------|------------|---------------------|
| Immediate | Single action        | Seconds    | Input, Feedback     |
| Session   | Connected actions    | Minutes    | State, Progress     |
| Extended  | Across sessions      | Days/Weeks | Persistence, Growth |
| Lifetime  | Product relationship | Ongoing    | Learning, Evolution |
```

---

#### üß© **Level 4: Systems** ‚Äî HOW (Implementation)

**Purpose:** Design documents for the "physics" of the product.

**Key Contents:**
- System boundaries and responsibilities
- Data structures and attributes
- Rules and algorithms (conceptual, not code)
- System interactions and interfaces
- State management

**Reference Rules:** Can reference L1-3 + sibling L4 docs + external. Cannot reference L5. Use placeholders for numbers.

**Suggested Structure:**
- `SYSTEMS_OVERVIEW.md` ‚Äî System interconnections
- `[DOMAIN]_SYSTEM.md` ‚Äî One per major subsystem

**Example:**
```markdown
## Entity Properties
- id (uuid), name (string), created_at (timestamp)
- status (enum: active, inactive, archived)
- metadata (key-value map)

## State Transitions
Entity moves from `active` to `inactive` when [condition].
Entity can only be `archived` if `inactive` for [duration].
```

---

#### ‚öñÔ∏è **Level 5: Implementation** ‚Äî Validation & Tuning

**Purpose:** Validate systems through user stories and tune with numeric parameters.

**Key Contents:**
- **User Stories** ‚Äî Validation from user perspective
  - Organized by priority/journey/system
  - Acceptance criteria + system references
  - Cross-cutting (spans multiple L4 systems)
- **Fine Tuning** ‚Äî Numeric balancing
  - Constants, timing, scaling factors
  - Rationale for each value
  - A/B test configurations

**Reference Rules:** Can reference ALL levels (1-4) + external. Only level that defines actual numeric values.

**Example User Story (YAML with Cypress Test DSL):**
```yaml
# @priority: MVP
# @systems: [TASK_SYSTEM, REWARD_SYSTEM]

id: US-102
title: See points awarded immediately on task completion

acceptance_criteria:
  - id: AC-102-1
    title: Points update within 100ms
    narrative: |
      Given I have an active task
      When I complete the task
      Then I see points increase within 100ms
    given:
      - loginAs: member
      - seedItem: { slug: 'my-task', status: 'active' }
      - visit: '/tasks/my-task'
    when:
      - click: { selector: '[data-test=complete-task]' }
    then:
      - shouldContain: { selector: '[data-test=points]', text: '10' }
```

**Example Fine Tuning:**
```yaml
# =============================================================================
# LEVEL 5: FINE TUNING - Interaction Timing
# =============================================================================
# @spec_version: 1.0.0
# @system: INTERACTION_TIMING
# @last_updated: 2024-01-15T14:30:00Z
# @status: production
# @spec_source: 03.INTERACTION_ARCHITECTURE.md

# All timing values in milliseconds

feedback:
  delay: 200
  # @rationale: Fast enough to feel immediate, slow enough to be noticeable
  # @reference: Nielsen Norman Group - Response Times: The 3 Important Limits
  # @constraints: min=50, max=500, type=integer

transitions:
  duration: 300
  # @rationale: Standard ease-out timing for smooth UI transitions
  # @alternatives: [200, 300, 500]
  # @test_result: 300ms rated highest for perceived smoothness (n=100)

timeouts:
  threshold: 30000
  # @rationale: Balance between patience and abandonment
  # @reference: UX research shows 30s is typical user patience limit
  # @constraints: min=5000, max=60000, type=integer
```

---

## üìê Document Reference Hierarchy

**CRITICAL RULE:** Each level can only reference higher levels, never lower ones.

### Reference Matrix

| Level                                  | Can Reference                                |
| -------------------------------------- | -------------------------------------------- |
| **Level 1** (Foundational Philosophy)  | External resources only                      |
| **Level 2** (Stable Truths)            | Level 1 + External                           |
| **Level 3** (Interaction Architecture) | Levels 1-2 + External                        |
| **Level 4** (Systems)                  | Levels 1-3 + Sibling Level 4 docs + External |
| **Level 5** (Implementation)           | All levels (1-4) + External                  |

### Enforcement Rules

**NEVER reference lower levels from higher levels.**

‚úó **Violations:**

- Level 3 referencing specific system implementations from Level 4
- Level 4 referencing numeric values from Level 5
- Level 2 referencing interaction patterns from Level 3

‚úì **Correct:**

- Level 2 referencing philosophical constraints from Level 1
- Level 4 referencing interaction principles from Level 3
- Level 5 referencing any level to understand context for tuning

### Why This Matters

This ensures:

- **Foundational documents remain stable** ‚Äî Philosophy doesn't change when you adjust numbers
- **Changes flow downward** ‚Äî Implementation evolves without corrupting principles
- **Architectural integrity** ‚Äî High-level decisions guide low-level details, not vice versa
- **Separation of concerns** ‚Äî What vs Why vs How vs How Much live at appropriate levels

---

## üìÅ User Project Directory Structure

When you create a specification using this framework, your project directory will contain:

```
/
‚îú‚îÄ‚îÄ 00.SPEC_FRAMEWORK.md               # Framework definition (copied reference)
‚îú‚îÄ‚îÄ 01.FOUNDATIONAL_PHILOSOPHY.md      # Your Level 1 spec (WHY & WHAT EXPERIENCE)
‚îú‚îÄ‚îÄ 02.STABLE_TRUTHS.md                # Design philosophy and commitments
‚îú‚îÄ‚îÄ 03.INTERACTION_ARCHITECTURE.md     # How users and product interact
‚îú‚îÄ‚îÄ 04.SYSTEMS/                        # Implementation architecture
‚îÇ   ‚îú‚îÄ‚îÄ SYSTEMS_OVERVIEW.md           # How systems interconnect
‚îÇ   ‚îú‚îÄ‚îÄ [SYSTEM_A].md                 # First major subsystem
‚îÇ   ‚îú‚îÄ‚îÄ [SYSTEM_B].md                 # Second major subsystem
‚îÇ   ‚îî‚îÄ‚îÄ [SYSTEM_N].md                 # Additional subsystems
‚îî‚îÄ‚îÄ 05.IMPLEMENTATION/                # Validation and tuning
    ‚îú‚îÄ‚îÄ USER_STORIES/                 # User perspective validation (YAML with Cypress tests)
    ‚îÇ   ‚îú‚îÄ‚îÄ USER_STORIES_OVERVIEW.md  # Organization and test generation docs
    ‚îÇ   ‚îú‚îÄ‚îÄ by_priority/              # Organized by development phase
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MVP/                  # MVP stories (create mvp.cy.ts to run)
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SECONDARY/            # Enhanced experience stories
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ADVANCED/             # Future feature stories
    ‚îÇ   ‚îú‚îÄ‚îÄ by_journey/               # Organized by user flow
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ [JOURNEY_A]/          # First user journey
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [JOURNEY_B]/          # Second user journey
    ‚îÇ   ‚îî‚îÄ‚îÄ by_system/                # Organized by system
    ‚îÇ       ‚îú‚îÄ‚îÄ [SYSTEM_A]/           # Stories for System A
    ‚îÇ       ‚îî‚îÄ‚îÄ [SYSTEM_B]/           # Stories for System B
    ‚îî‚îÄ‚îÄ FINE_TUNING/                  # Numeric balancing (comment-annotated YAML)
        ‚îú‚îÄ‚îÄ [SYSTEM_A].yml            # Parameters for System A
        ‚îú‚îÄ‚îÄ [SYSTEM_B].yml            # Parameters for System B
        ‚îî‚îÄ‚îÄ [PARAMETERS_N].yml        # Additional parameter sets
```

---

## Usage Guidelines

### When Creating New Documents

1. **Determine the correct level** ‚Äî Ask: "Is this about WHY, WHAT strategy, HOW it works conceptually, HOW it's implemented, or HOW MUCH?"
2. **Check reference constraints** ‚Äî Ensure you only reference appropriate levels
3. **Use placeholders** ‚Äî In Levels 1-4, use descriptive placeholders like `[short duration]` instead of `200ms`
4. **Maintain abstraction** ‚Äî Each level should be complete without depending on lower levels

<!-- SECTION: Level 5 USER_STORIES YAML Format -->

### Organizing User Stories at Level 5

User stories belong at **Level 5** (Implementation), where they validate system implementations from the user's perspective and automatically generate Cypress tests. User stories are written in **YAML format** with structured acceptance criteria that map directly to executable test specifications.

**Why Level 5?**

- User stories validate that systems deliver the intended user experience
- They bridge the gap between system specifications (Level 4) and actual implementation
- They're cross-cutting by nature, often spanning multiple systems
- They provide executable acceptance criteria through auto-generated Cypress tests
- They maintain traceability from user intent to test validation

**Core principles:**

- Stories focus on **user outcomes** while providing **testable specifications**
- Stories tell **what** and **why** through narrative, **how to test** through DSL steps
- Acceptance criteria define **observable behaviors** in machine-parseable format
- Stories should be specific enough to implement and test unambiguously
- Stories can reference multiple Level 4 systems (cross-cutting)
- Include system references to link back to architectural specs
- **Stories describe meaningful user value‚Äîprimarily golden paths and important edge cases.** Do not create user stories for every behavior or internal system detail.

**Organization structure:**

User stories are organized in YAML files across collections that support different perspectives:

1. **by_priority/** ‚Äî Groups stories by development phase (MVP, Secondary, Advanced)

   - Helps prioritize implementation efforts
   - Aligns with product roadmap and release planning

2. **by_journey/** ‚Äî Groups stories by user flow (e.g., Onboarding, Daily Usage)

   - Validates complete user experiences end-to-end
   - Ensures coherent user journeys across systems

3. **by_system/** ‚Äî Groups stories by primary system referenced
   - Links validation back to system architecture
   - Helps verify each system delivers user value

Stories can appear in multiple collections to enable different test perspectives.

**Test Suite Files:**

For each collection or subset you want to run independently, create a Cypress test suite file:

1. Copy `cypress/e2e/example.cy.ts` to a new file (e.g., `mvp.cy.ts`, `onboarding.cy.ts`)
2. Modify the glob pattern to load the specific YAML files for that suite
3. Run tests with: `cypress run --spec 'cypress/e2e/mvp.cy.ts'`

Example test suites:
- `mvp.cy.ts` ‚Üí loads `by_priority/MVP/**/*.yaml`
- `onboarding.cy.ts` ‚Üí loads `by_journey/ONBOARDING/**/*.yaml`
- `tasks.cy.ts` ‚Üí loads `by_system/TASKS/**/*.yaml`
- `all-tests.cy.ts` ‚Üí loads `**/*.yaml` (everything)

**YAML Format with Test DSL:**

User stories use comment-annotated YAML format similar to FINE_TUNING files:

```yaml
# =============================================================================
# USER STORY: [Story Title]
# =============================================================================
# @spec_version: 1.0.0
# @priority: MVP | SECONDARY | ADVANCED
# @journey: [JOURNEY_NAME]
# @systems: [SYSTEM_A, SYSTEM_B]
# @last_updated: [ISO 8601 timestamp]

id: US-101
title: [Story title]
requirement_id: R-101  # Optional reference to requirements

acceptance_criteria:
  - id: AC-101-1
    title: [Acceptance criteria title]
    narrative: |
      Given [context]
      When [action]
      Then [expected outcome]

    # Test DSL: Given-When-Then structure
    given:
      - [setup steps]
    when:
      - [action step]
    then:
      - [assertion steps]
```

**YAML Syntax Requirements:**

User stories are written in YAML format, which has specific syntax rules for handling multi-line text and special characters. Following these rules ensures your YAML files parse correctly and generate valid Cypress tests.

**Use Block Scalars for Multi-line Text:**

When a field contains multiple lines or includes colons, use the pipe operator (`|`) to create a block scalar:

```yaml
# ‚úÖ CORRECT: Using pipe operator for multi-line text
narrative: |
  Given I am a logged-in member viewing the main screen
  When I click the add task button and enter a task name
  Then I should see the task appear in my current list

# ‚úÖ CORRECT: Using pipe operator for text with colons
notes: |
  This is a PLACEHOLDER file. Advanced features are intentionally deferred until:
  1. MVP and Secondary features are proven and stable
  2. Clear market demand from growing user base

# ‚ùå WRONG: Unquoted multi-line text (will fail to parse)
narrative:
  Given I am a logged-in member viewing the main screen
  When I click the add task button

# ‚ùå WRONG: Unquoted text with colons (will fail to parse)
notes: Some context: this will break parsing here: because of colons
```

**Handling Quotes and Apostrophes:**

YAML has specific rules for escaping quotes within strings. Follow these guidelines to avoid parsing errors:

```yaml
# ‚úÖ CORRECT: Use double quotes for strings with apostrophes
- shouldAlert: { studentId: 123, message: "Expected to return but hasn't attended" }
- text: "It's a beautiful day"

# ‚úÖ CORRECT: Escape single quotes by doubling them
- shouldAlert: { studentId: 123, message: 'Expected to return but hasn''t attended' }
- text: 'It''s a beautiful day'

# ‚úÖ CORRECT: Use double quotes for strings with double quotes (escape with \)
- message: "She said \"hello\" to me"

# ‚ùå WRONG: Backslash escape in single quotes (invalid YAML syntax)
- shouldAlert: { studentId: 123, message: 'Expected to return but hasn\'t attended' }

# ‚ùå WRONG: Unescaped double quotes inside double quotes
- message: "She said "hello" to me"
```

**Best practice:** Use double quotes for strings containing apostrophes - it's cleaner and more readable than doubling single quotes.

**Common YAML Pitfalls:**

1. **Unquoted strings with colons** - Any text containing `:` must use block scalar or quotes
2. **Invalid quote escaping** - Cannot use `\'` inside single quotes; use double quotes or double the single quote (`''`)
3. **Improper indentation** - YAML requires consistent indentation (use 2 spaces)
4. **Special characters** - Characters like `{`, `}`, `[`, `]`, `,`, `&`, `*`, `#`, `?`, `|`, `-`, `<`, `>`, `=`, `!`, `%`, `@`, `` ` `` need proper quoting or block scalars
5. **Missing block indicator** - Multi-line text needs `|` or `>` after the field name
6. **Empty array elements** - Extra dashes (`-`) with no content create null values that cause validation errors

**Quick Reference:**

- **Multi-line text**: Always use `field: |` followed by indented content
- **Text with colons**: Use `field: |` or wrap in quotes: `field: "text: with: colons"`
- **Text with apostrophes**: Use double quotes: `message: "hasn't"` (preferred) or double the quote: `message: 'hasn''t'`
- **Array elements**: Each dash must have content; ensure dashes are aligned at same column
- **Indentation**: Use 2 spaces consistently (never tabs)
- **Comments**: Start with `#` and can appear on their own line or after values
- **Annotations**: Use `# @annotation: value` format for metadata

**When in doubt**, use the pipe operator - it handles all edge cases correctly.

**Troubleshooting YAML Validation Errors:**

When you encounter validation errors during test generation, the error message will include a path like `acceptance_criteria[2].then[1]` indicating where the problem occurs.

**How to locate the error:**
1. `acceptance_criteria[X]` = The Xth acceptance criterion in your story (0-indexed, so [2] = 3rd criterion)
2. `.then[Y]` = The Yth step in the `then` array (0-indexed, so [1] = 2nd assertion step)
3. Count through your YAML file to find the exact location

**Common error: "invalid_union" (all union members failed)**

This error means a step value doesn't match ANY expected type. Usually caused by:

```yaml
# ‚ùå WRONG: Empty array element (most common cause)
then:
  - shouldExist: { selector: '[data-test=button]' }
  -    # Extra dash with no content = null
  - shouldContain: { selector: '[data-test=message]', text: 'Success' }

# ‚úÖ CORRECT: Remove the empty element
then:
  - shouldExist: { selector: '[data-test=button]' }
  - shouldContain: { selector: '[data-test=message]', text: 'Success' }

# ‚ùå WRONG: Incorrect indentation (creates nested structure)
then:
  - shouldExist: { selector: '[data-test=button]' }
    - shouldContain: { selector: '[data-test=message]', text: 'Success' }
  #^ This dash is indented too far

# ‚úÖ CORRECT: Align all dashes at same column
then:
  - shouldExist: { selector: '[data-test=button]' }
  - shouldContain: { selector: '[data-test=message]', text: 'Success' }

# ‚ùå WRONG: String instead of object
then:
  - shouldExist: { selector: '[data-test=button]' }
  - "Check that success message appears"

# ‚úÖ CORRECT: Use proper step object format
then:
  - shouldExist: { selector: '[data-test=button]' }
  - shouldContain: { selector: '[data-test=message]', text: 'Success' }
```

**Validation checklist:**
- [ ] No extra dashes (`-`) without content following them
- [ ] All sibling array elements have dashes aligned at same column
- [ ] Each step is an object: `{ stepName: value }` or `{ stepName: { properties } }`
- [ ] No null, undefined, or string values where objects are expected
- [ ] All required properties present on each step (e.g., `shouldContain` needs both `selector` and `text`)

**Core Test DSL Steps:**

The framework provides a core set of test steps that can be extended with domain-specific steps:

**Setup Steps (given/when):**
- `visit: '/path'` - Navigate to a path
- `click: { selector: '[data-test=button]' }` - Click an element
- `fill: { selector: '[data-test=input]', value: 'text' }` - Fill an input
- `loginAs: 'user_role'` - Authenticate as a user type
- `seedItem: { slug: 'item-id', status: 'available' }` - Seed test data

**Assertion Steps (then):**
- `shouldContain: { selector: '[data-test=element]', text: 'expected' }` - Verify text content
- `shouldExist: { selector: '[data-test=element]' }` - Verify element exists

**Extending the DSL:**

Projects should extend the core DSL with domain-specific steps in `templates/cypress/support/steps.ts`:

```typescript
// Add custom step types to the Step union
export type Step =
  | { visit: string }
  | { click: { selector: string } }
  // ... core steps ...
  | { createProject: { name: string } }  // Custom step
  | { inviteUser: { email: string, role: string } }  // Custom step

// Implement custom steps in runSetupSteps
export function runSetupSteps(steps: Step[]) {
  for (const s of steps ?? []) {
    if ('visit' in s) cy.visit(s.visit);
    // ... other steps ...
    else if ('createProject' in s) {
      cy.task('createProject', s.createProject);
    }
    else if ('inviteUser' in s) {
      cy.task('inviteUser', s.inviteUser);
    }
  }
}
```

**Example User Story:**

```yaml
# =============================================================================
# USER STORY: Quick Task Creation
# =============================================================================
# @spec_version: 1.0.0
# @priority: MVP
# @journey: DAILY_USAGE
# @systems: [TASK_SYSTEM]
# @last_updated: 2025-11-09T00:00:00Z

id: US-101
title: Add new tasks quickly
requirement_id: R-101

acceptance_criteria:
  - id: AC-101-1
    title: Member can create task from main screen
    narrative: |
      Given I am a logged-in member viewing the main screen
      When I click the add task button and enter a task name
      Then I should see the task appear in my current list
    given:
      - loginAs: member
      - visit: '/dashboard'
    when:
      - click: { selector: '[data-test=add-task]' }
      - fill: { selector: '[data-test=task-input]', value: 'Buy groceries' }
      - click: { selector: '[data-test=save-task]' }
    then:
      - shouldExist: { selector: '[data-test=task-list] [data-test=task-item]' }
      - shouldContain: { selector: '[data-test=task-item]', text: 'Buy groceries' }

  - id: AC-101-2
    title: Empty tasks are rejected
    narrative: |
      Given I am a logged-in member viewing the main screen
      When I try to create a task with only whitespace
      Then I should see a validation error and no task is created
    given:
      - loginAs: member
      - visit: '/dashboard'
    when:
      - click: { selector: '[data-test=add-task]' }
      - fill: { selector: '[data-test=task-input]', value: '   ' }
      - click: { selector: '[data-test=save-task]' }
    then:
      - shouldContain: { selector: '[data-test=error]', text: 'Task name is required' }
      - shouldExist: { selector: '[data-test=task-list]:empty' }
```

**Cross-System Example:**

```yaml
# =============================================================================
# USER STORY: Instant Reward Feedback
# =============================================================================
# @spec_version: 1.0.0
# @priority: MVP
# @journey: DAILY_USAGE
# @systems: [TASK_SYSTEM, REWARD_SYSTEM, VIEW_SYSTEM]
# @last_updated: 2025-11-09T00:00:00Z

id: US-102
title: See points awarded immediately on task completion
requirement_id: R-102

acceptance_criteria:
  - id: AC-102-1
    title: Points update within 100ms of completion
    narrative: |
      Given I have an active task with base points value
      When I mark the task as complete
      Then I should see my points total increase within 100ms
    given:
      - loginAs: member
      - seedItem: { slug: 'my-task', status: 'active', points: 10 }
      - visit: '/tasks/my-task'
    when:
      - click: { selector: '[data-test=complete-task]' }
    then:
      - shouldContain: { selector: '[data-test=points-total]', text: '10' }
      - shouldExist: { selector: '[data-test=feedback-animation]' }
```

**Test Control Modifiers (skip/only):**

User stories and acceptance criteria support optional `skip` and `only` modifiers to control which tests run during development:

```yaml
# Story-level modifiers affect all acceptance criteria in the story
id: US-101
title: Add new tasks quickly
skip: true  # Skip this entire story
# only: true  # Or run ONLY this story (comment out when not needed)

acceptance_criteria:
  - id: AC-101-1
    title: Member can create task from main screen
    # skip: true  # Skip just this acceptance criterion
    # only: true  # Or run ONLY this acceptance criterion
    narrative: |
      Given I am a logged-in member viewing the main screen
      When I click the add task button and enter a task name
      Then I should see the task appear in my current list
    given:
      - loginAs: member
    when:
      - click: { selector: '[data-test=add-task]' }
    then:
      - shouldExist: { selector: '[data-test=task-list]' }
```

**Modifier Behavior:**

- **skip:** Marks tests as skipped - they appear in test output but don't run
- **only:** Runs ONLY tests marked with `only` - all other tests are skipped
- **Multiple only:** If multiple stories/ACs have `only: true`, ALL of them run (additive behavior, matching Cypress `.only()`)

**Modifier Precedence:**

AC-level modifiers work within focused stories, allowing fine-grained test control:

1. **AC-level `only` within `story.only`:** If a story has `only: true` and one AC has `only: true`, ONLY that AC runs
2. **AC-level `skip` within `story.only`:** Individual ACs can be skipped even when the story is focused
3. **Story-level `skip`:** Suppresses all AC-level modifiers (can't run tests in a skipped story)
4. **Story-level `only`:** Focuses the story but allows AC-level modifiers to work within it

**Best Practices:**

1. **Use skip sparingly** - Skipped tests can be forgotten. Consider deleting or moving to POST_MVP instead
2. **Never commit only** - The `only` modifier is for local development only. Remove before committing
3. **Document skip reasons** - Add a comment explaining why a test is skipped
4. **Prefer story-level modifiers** - Skip entire stories when debugging rather than individual ACs

**Example with modifiers:**

```yaml
id: US-103
title: Advanced search with filters
skip: true  # Skipping - pending backend search API implementation (GH issue #42)

acceptance_criteria:
  - id: AC-103-1
    title: Filter by date range
    narrative: |
      Given I am viewing the search page
      When I select a date range filter
      Then I should see only results within that range
    # This AC will be skipped because the story-level skip takes precedence
    given:
      - loginAs: member
      - visit: '/search'
    when:
      - click: { selector: '[data-test=date-filter]' }
    then:
      - shouldExist: { selector: '[data-test=filtered-results]' }
```

**Runtime Test Generation:**

User story YAML files are transformed into Cypress tests at runtime by test suite files:

- **Schema validation:** `cypress/support/schema.ts` validates YAML structure
- **DSL interpretation:** `cypress/support/steps.ts` converts DSL to Cypress commands
- **Test suites:** Each `.cy.ts` file loads YAML via glob pattern and generates tests

**Directory Structure:**

```
05.IMPLEMENTATION/
‚îî‚îÄ‚îÄ USER_STORIES/
    ‚îú‚îÄ‚îÄ USER_STORIES_OVERVIEW.md     # Organization docs
    ‚îú‚îÄ‚îÄ by_priority/
    ‚îÇ   ‚îú‚îÄ‚îÄ MVP/                     # MVP user stories
    ‚îÇ   ‚îú‚îÄ‚îÄ SECONDARY/               # Secondary features
    ‚îÇ   ‚îî‚îÄ‚îÄ ADVANCED/                # Advanced features
    ‚îú‚îÄ‚îÄ by_journey/
    ‚îÇ   ‚îú‚îÄ‚îÄ ONBOARDING/              # Onboarding journey
    ‚îÇ   ‚îî‚îÄ‚îÄ DAILY_USAGE/             # Daily usage journey
    ‚îî‚îÄ‚îÄ by_system/
        ‚îú‚îÄ‚îÄ TASKS/                   # Task system stories
        ‚îî‚îÄ‚îÄ REWARDS/                 # Reward system stories

cypress/e2e/
‚îú‚îÄ‚îÄ example.cy.ts                    # Template - copy and customize
‚îú‚îÄ‚îÄ mvp.cy.ts                        # Loads by_priority/MVP/**/*.yaml
‚îú‚îÄ‚îÄ onboarding.cy.ts                 # Loads by_journey/ONBOARDING/**/*.yaml
‚îî‚îÄ‚îÄ tasks.cy.ts                      # Loads by_system/TASKS/**/*.yaml
```

**Benefits of YAML + Cypress Approach:**

- **Single source of truth:** User stories ARE the test specification
- **Living documentation:** Tests always match documented user stories
- **Traceability:** Clear path from L4 systems ‚Üí L5 stories ‚Üí Cypress tests
- **Version control:** YAML stories track changes with full context
- **AI-friendly:** Structured format for AI-assisted story/test generation
- **Multi-view testing:** Same stories generate priority, journey, and system test suites

---

<!-- SECTION: Level 5 FINE_TUNING YAML Format -->

### Fine Tuning YAML Format and Conventions

Fine tuning parameters at Level 5 use **comment-annotated YAML** format. This approach keeps the YAML structure clean (containing only actual values) while embedding rich metadata and rationale in comments.

**Core Principles:**

1. **Values are first-class** ‚Äî The YAML structure contains only deployable configuration values
2. **Metadata in comments** ‚Äî All annotations, rationale, and context use comment syntax
3. **Machine-parseable** ‚Äî Strip comments to get clean config; parse comments for documentation
4. **Human-readable** ‚Äî Rich context without cluttering the data structure
5. **Traceable** ‚Äî Every value links back to spec levels and decisions

**Comment Annotation Syntax:**

Use `# @annotation: value` format for structured metadata:

```yaml
# =============================================================================
# LEVEL 5: FINE TUNING - [System Name]
# =============================================================================
# @spec_version: [version]
# @system: [SYSTEM_NAME]
# @last_updated: [ISO 8601 timestamp]
# @status: draft | testing | production | deprecated
# @spec_source: [reference to L1-4 docs]

# Section description (free-form comment)
# @spec_source: [specific doc reference for this section]

parameter_name: value
# @rationale: Why this specific value was chosen
# @reference: External research, papers, or documentation
# @alternatives: [other values considered]
# @test_result: Supporting data from testing
# @constraints: min=X, max=Y, type=Z
# @inviolable: true (if from L1, cannot be changed)
# @review_date: When to revisit this value
# @impact: [affected systems]
```

**Standard Annotation Tags:**

| Tag | Purpose | Example |
|-----|---------|---------|
| `@spec_version` | Version of spec this implements | `@spec_version: 1.0.0` |
| `@system` | Which L4 system this configures | `@system: REWARD_SYSTEM` |
| `@last_updated` | ISO 8601 timestamp | `@last_updated: 2024-01-15T14:30:00Z` |
| `@status` | Current status | `@status: production` |
| `@spec_source` | Reference to spec docs | `@spec_source: 04.SYSTEMS/REWARD_SYSTEM.md:45` |
| `@rationale` | Why this value | `@rationale: Testing showed optimal engagement` |
| `@reference` | External sources | `@reference: Nielsen Norman Group - Response Times` |
| `@alternatives` | Other values tested | `@alternatives: [5, 10, 20]` |
| `@test_result` | Supporting data | `@test_result: 82% engagement (n=100)` |
| `@constraints` | Validation rules | `@constraints: min=1, max=100, type=integer` |
| `@inviolable` | Cannot change (L1) | `@inviolable: true` |
| `@review_date` | When to revisit | `@review_date: 2024-04-15` |
| `@impact` | Affected systems | `@impact: [REWARD_SYSTEM, PROGRESS_SYSTEM]` |
| `@ab_test` | Experiment config | `@ab_test: variants=[0.05, 0.07, 0.10], duration=4w` |
| `@ethical_review` | Ethical considerations | `@ethical_review: Approved 2024-01-08, see ethics doc` |

**Complete Example:**

```yaml
# =============================================================================
# LEVEL 5: FINE TUNING - Reward System
# =============================================================================
# @spec_version: 1.0.0
# @system: REWARD_SYSTEM
# @last_updated: 2024-01-15T14:30:00Z
# @status: production
# @spec_source: 04.SYSTEMS/REWARD_SYSTEM.md

# -----------------------------------------------------------------------------
# Point Calculation
# -----------------------------------------------------------------------------
# @spec_source: 04.SYSTEMS/REWARD_SYSTEM.md:45

points:
  base: 10
  # @rationale: After testing with 100 users over 2 weeks:
  #   - 5 points: felt insignificant, engagement dropped 23%
  #   - 10 points: "just right", engagement peaked at 82%
  #   - 20 points: felt cheap, users hit 1000pts in 3 days
  # @test_result: engagement_score=8.7/10 (n=100, 2024-01-10)
  # @alternatives: [5, 15, 20, 25]
  # @constraints: min=1, max=100, type=integer
  # @review_date: 2024-04-15
  # @impact: [REWARD_SYSTEM, PROGRESS_SYSTEM, ACHIEVEMENT_SYSTEM]

  difficulty_multipliers:
    easy: 1.0
    medium: 1.5
    hard: 2.0
    # @rationale: 50% increase per tier follows Weber-Fechner law of
    #   just-noticeable difference in human perception
    # @reference: https://en.wikipedia.org/wiki/Weber-Fechner_law
    # @spec_source: 04.SYSTEMS/REWARD_SYSTEM.md:52
    # @alternatives: linear=[10,15,20], exponential=[1.0,2.0,4.0]
    # @test_result: 50% intervals rated clearest (clarity=8.9/10, n=500)

# -----------------------------------------------------------------------------
# Variable Rewards (Bonus System)
# -----------------------------------------------------------------------------
# @spec_source: 02.STABLE_TRUTHS.md:78, 04.SYSTEMS/REWARD_SYSTEM.md:89

variable_rewards:
  probability: 0.07
  # @rationale: Variable ratio schedule from Skinner's research.
  #   7% = ~1 bonus per 14 completions, ~1 every 3-4 days for typical users
  # @reference: Skinner (1956) - operant conditioning research
  # @alternatives: [0.05, 0.07, 0.10]
  # @test_result: 0.05 too rare, 0.10 too predictable, 0.07 optimal
  # @ab_test: variants=[0.05, 0.07, 0.10], duration=4w, metric=retention
  # @ethical_review: Approved 2024-01-08 - transparent, no punishment, optional
  # @constraints: min=0.01, max=0.20, type=float

  multiplier:
    min: 1.5
    max: 3.0
    # @rationale: 1.5x minimum feels special, 3.0x max avoids game-breaking
    # @spec_source: 04.SYSTEMS/REWARD_SYSTEM.md:94
    # @alternatives: conservative=[1.2,2.0], aggressive=[2.0,5.0]
    # @test_result: [1.5,3.0] rated "exciting but fair" (n=50)
    # @constraints: min=1.1, max=5.0, type=float
    # @review_date: 2024-07-15

# -----------------------------------------------------------------------------
# Inviolable Constraints (from L1 Foundational Philosophy)
# -----------------------------------------------------------------------------
# @spec_source: 01.FOUNDATIONAL_PHILOSOPHY.md

streak:
  enabled: false
  # @inviolable: true
  # @rationale: L1 principle "Never create guilt" - no punishment for breaks
  # @spec_source: 01.FOUNDATIONAL_PHILOSOPHY.md:23
  # @note: Streak bonuses available in code but disabled per L1 philosophy
```

**Benefits:**

- **Clean YAML** ‚Äî Strip comments ‚Üí deployable config
- **Rich documentation** ‚Äî All context inline with values
- **Traceability** ‚Äî Every value traces to spec decisions
- **Version control** ‚Äî Git diffs show values + context changes
- **Validation** ‚Äî `@constraints` enable automated validation
- **Tooling-friendly** ‚Äî Parse `@annotations` for automation

**Tooling Recommendations:**

1. **Config Generator** ‚Äî Strip all comments to produce clean deployment configs
2. **Documentation Generator** ‚Äî Extract `@rationale` and `@reference` to create decision logs
3. **Validator** ‚Äî Parse `@constraints` to validate values
4. **Traceability Tool** ‚Äî Follow `@spec_source` back to architectural docs
5. **Change Impact Analyzer** ‚Äî Use `@impact` to assess change ripple effects

<!-- SECTION: Root Extensions -->

### Root Extensions: Specialized Artifacts from Specification

A mature specification can produce specialized artifacts through **root extensions**.
Like mycorrhizal networks in nature, these extensions connect to your root system
(specification) and extract specialized nutrients that feed your product tree.

**The Extension Metaphor:**
- Spec (L1-L5) = root system (underground foundation)
- Extensions = specialized root networks extending from specific levels
- Product = tree (grows stronger from extended root system's nutrients)

**Extension Anatomy:**

Every extension type follows this pattern:

| Element          | Purpose                                           |
| ---------------- | ------------------------------------------------- |
| **Source**       | Which spec level(s) to read                       |
| **Context**      | Additional info AI needs (tech stack, conventions)|
| **Transformation** | What specialized artifact to produce           |
| **Output Format** | Structure of produced artifact                   |
| **Traceability** | How to link back to source spec                  |

**Available Extension Types:**

| Extension Type                    | Input             | Output                                        | Dependencies  |
| --------------------------------- | ----------------- | --------------------------------------------- | ------------- |
| **Systems ‚Üí Technical Design**    | L4 Systems        | Architecture diagrams, API specs, data models | ‚Äî             |
| **User Stories ‚Üí UX Design**      | L5 USER_STORIES   | Wireframes, user flows, screen specs          | ‚Äî             |
| **UX Design ‚Üí UI Design**         | UX Design doc     | Visual specs, component library, design tokens | ux-design     |
| **Philosophy ‚Üí Brand Guidelines** | L1 Design Pillars | Voice/tone guide, brand principles            | ‚Äî             |
| **Interaction ‚Üí Analytics Plan**  | L3 Patterns       | Event taxonomy, tracking specifications       | ‚Äî             |
| **Fine-Tuning ‚Üí Config Schema**   | L5 FINE_TUNING    | JSON schemas, validation rules                | ‚Äî             |

**CLI Usage:**

```bash
# List available extension types
rootspec extend

# Core extensions (start here)
rootspec extend technical-design   # L4 ‚Üí Architecture & APIs
rootspec extend ux-design           # L5 ‚Üí Wireframes & flows
rootspec extend brand-guidelines    # L1 ‚Üí Voice & tone

# Additional extensions
rootspec extend ui-design           # UX ‚Üí Visual specs (requires ux-design first)
rootspec extend analytics-plan      # L3 ‚Üí Event taxonomy
rootspec extend config-schema       # L5 ‚Üí JSON Schema
```

Each extension generates a contextualized AI prompt that:
1. Auto-detects your spec structure
2. Fills in project-specific details
3. Provides prescriptive multi-phase workflow
4. Ensures traceability back to source

### When Modifying Documents

1. **Start at the appropriate level** ‚Äî If changing philosophy, start at Level 1; if tuning numbers, work in Level 5
2. **Propagate changes downward** ‚Äî Update lower levels to align with changes in higher levels
3. **Never propagate upward** ‚Äî Implementation details should never require changes to philosophy
4. **Verify references** ‚Äî Ensure no new violations of the hierarchy rules

### When Reviewing Documents

- **Check for leaky abstractions** ‚Äî Does a high-level doc reference implementation details?
- **Verify single source of truth** ‚Äî Is each concept defined at exactly one level?
- **Confirm stability** ‚Äî Could you completely reimplement Level 4 without touching Levels 1-3?

---

## Adaptation Guide

To adapt this framework to your project:

1. **Keep the 5-level structure** ‚Äî The hierarchy has proven effective across diverse domains
2. **Rename Level 3 if needed** ‚Äî "Interaction Architecture" works for many products, but you might use "Behavioral Architecture," "Information Architecture," "Flow Architecture," etc.
3. **Customize Level 4 subsystems** ‚Äî Your systems will be specific to your domain
4. **Organize Level 5 appropriately** ‚Äî Use USER_STORIES/ for validation and FINE_TUNING/ for parameters, or adapt the structure to your implementation needs
5. **Maintain reference rules** ‚Äî This is the core constraint that makes the framework valuable
6. **Add a project-specific guide** ‚Äî Consider creating a `CLAUDE.md` or similar to guide AI assistants on your specific terminology and conventions

---

<!-- SECTION: AI Assistant Guidance -->

## AI Assistant Guidance for Creating New Specs

### Overview

This section provides guidance for AI assistants helping product owners create specifications using this framework. The goal is to extract complete, well-structured information through strategic questioning while maintaining the hierarchical integrity of the spec system.

**Core Principles for AI Assistants:**

1. **Always start at Level 1** ‚Äî Resist jumping to implementation details; understand the "why" first
2. **One level at a time** ‚Äî Complete each level before descending to the next
3. **Ask "why" more than "what"** ‚Äî Understanding intent prevents misaligned implementations
4. **Use concrete examples** ‚Äî Request specific scenarios and user stories, not abstractions
5. **Validate hierarchy** ‚Äî Continuously check that levels don't reference lower levels
6. **Challenge assumptions** ‚Äî Question unstated dependencies and hidden implementation details
7. **Identify anti-patterns** ‚Äî Help users avoid common specification mistakes

---

### Session Flow Recommendations

Effective spec creation typically follows this phased approach:

#### **Phase 1: Discovery ‚Äî Level 1**

**Objective:** Extract the philosophical and emotional foundation

- Focus exclusively on WHY the product must exist
- Ask multiple rounds of clarifying questions
- Challenge the user's assumptions about problems and solutions
- Identify what the product will NOT do
- Define emotional goals and user promises
- Validate that principles are truly inviolable

**Key milestone:** Product owner can articulate the mission without mentioning features

---

#### **Phase 2: Strategy ‚Äî Level 2**

**Objective:** Translate principles into design commitments

- Identify frameworks, paradigms, or existing patterns that apply
- Document architectural philosophies and why they were chosen
- Define success criteria and constraints
- Explore trade-offs and their justifications
- Identify what approaches are being rejected and why

**Key milestone:** Clear design philosophy that guides all subsequent decisions

---

#### **Phase 3: Architecture ‚Äî Level 3**

**Objective:** Map the interaction model before implementation

- Walk through complete user journeys from trigger to outcome
- Identify all interaction loops and their scales (immediate, session, extended, lifetime)
- Document how different parts coordinate to create experiences
- Explore failure modes and edge cases
- Define verification criteria for implementations

**Key milestone:** Complete interaction architecture that could guide multiple different implementations

---

#### **Phase 4: Systems ‚Äî Level 4**

**Objective:** Define system boundaries, data, and rules

- Break product into logical subsystems
- Define each system's responsibilities and boundaries
- Document data models with attributes and types
- Specify rules, algorithms, and state transitions (conceptually, not code)
- Map system interactions and dependencies
- Use placeholders for all numeric values

**Key milestone:** Complete system specifications that implementers can build from

---

#### **Phase 5: Validation ‚Äî Level 5**

**Objective:** Create user stories and identify tuning parameters

- Generate user stories from the user's perspective
- Organize stories by priority (MVP/Secondary/Advanced), journey, and system
- Define observable acceptance criteria
- Identify numeric parameters that need tuning
- Document rationale for parameter choices

**Key milestone:** User stories that validate systems deliver the intended experience

---

### Level-by-Level Question Templates

Use these questions to guide product owners through each level. Adapt based on context.

#### **Level 1: Foundational Philosophy**

1. **"What problem does this product solve, and why must it exist?"** ‚Äî Focus on user pain and transformation, not features (Mission)
2. **"What 3-5 core experiences or emotions should users feel?"** ‚Äî These become your Design Pillars
3. **"For each pillar, what specific feeling are you creating?"** ‚Äî Joy? Empowered? Calm? Confident? Be specific.
4. **"What principles would you never violate, even if it cost features or users?"** ‚Äî Identify inviolable constraints
5. **"Who is this NOT for? What will this product NOT do?"** ‚Äî Define boundaries through exclusion
6. **"If this product succeeds, how will users' lives be different in 6 months?"** ‚Äî Validate long-term impact (North-star)

**Warning Signs for Design Pillars:**
- Too many pillars (>8): Dilutes focus, pick the core 3-5
- Feature-specific: "Combat system" is too narrow; "Empowered Action" is better
- Too generic: "Make users happy" applies to everything
- Implementation details: "React-based architecture" doesn't belong here

#### **Level 2: Stable Truths**

1. **"What design philosophy or framework guides your approach?"** ‚Äî Identify existing paradigms or create new ones
2. **"What are you optimizing for, and what trade-offs will you accept?"** ‚Äî Surface competing values
3. **"What patterns from other domains apply here?"** ‚Äî Game design? Education? Social systems?
4. **"What existing approaches are you rejecting, and why?"** ‚Äî Learn from what NOT to do
5. **"How do you define success for this product?"** ‚Äî Metrics, user behaviors, outcomes

#### **Level 3: Interaction Architecture**

1. **"Walk me through a complete user journey from start to finish"** ‚Äî Get concrete flow, not abstract description
2. **"What triggers each interaction, and what feedback does the user receive?"** ‚Äî Map the complete loop
3. **"Are there different scales of interaction? Immediate? Session-level? Cross-session?"** ‚Äî Identify temporal architecture
4. **"What happens when things fail or users skip steps?"** ‚Äî Explore edge cases early
5. **"How do different systems coordinate to create coherent experiences?"** ‚Äî Understand cross-system orchestration

#### **Level 4: Systems**

1. **"What are the major subsystems, and what is each responsible for?"** ‚Äî Establish boundaries
2. **"What data does each system manage? What are the key entities?"** ‚Äî Define information architecture
3. **"What rules govern state transitions and system behavior?"** ‚Äî Extract business logic
4. **"How do systems interact? What do they expose to each other?"** ‚Äî Map interfaces and dependencies
5. **"What values are calculated, and from what inputs?"** ‚Äî Identify derived fields and formulas

#### **Level 5: User Stories**

1. **"As a user, what would you want to accomplish in the first 5 minutes? First day? First week?"** ‚Äî Extract temporal user needs
2. **"For each feature, what observable behaviors would confirm it's working?"** ‚Äî Define acceptance criteria
3. **"Which features are absolutely essential (MVP) vs. nice-to-have vs. future?"** ‚Äî Establish priority
4. **"What are the complete user journeys from entry to exit?"** ‚Äî Map end-to-end flows
5. **"How would you know if the system is delivering the intended experience?"** ‚Äî Validate alignment with Level 1

#### **Level 5: Fine Tuning**

1. **"What timing values matter? Response delays? Animation durations?"** ‚Äî Identify temporal parameters
2. **"What numeric thresholds determine state changes?"** ‚Äî Find decision boundaries
3. **"What scaling factors affect calculations?"** ‚Äî Identify growth curves and multipliers
4. **"What are the rationales for these specific numbers?"** ‚Äî Document reasoning for future tuning
5. **"How might these values change based on testing or feedback?"** ‚Äî Plan for iteration

---

### Warning Signs & Anti-Patterns

Watch for these common mistakes and redirect appropriately:

| Level | Anti-Pattern | Example | Redirect Question |
|-------|-------------|---------|-------------------|
| **L1** | Features instead of purpose | "Points system and badges" | "Why must this product exist? What transformation?" |
| **L1** | Implementation constraints in principles | "React and microservices" | "What philosophical principles guide the experience?" |
| **L1** | Vague emotional goals | "Users should feel good" | "Specific emotion for Design Pillar? Empowered? Calm? Accomplished?" |
| **L1** | Too many design pillars | "8 different core experiences" | "What are the 3-5 MOST important experiences that define this product?" |
| **L2** | Referencing system implementations | "Task System uses priority queue" | "What's the design philosophy about task management?" |
| **L2** | Including numeric parameters | "Tasks worth 100 points" | "What's the strategic approach to rewards?" |
| **L2** | Mixing strategy with interaction | "Celebration animation on completion" | "What's the stable truth about feedback?" |
| **L3** | Jumping to UI/UX implementation | "Button in top-right corner" | "What interaction is the user performing?" |
| **L3** | Referencing system logic | "Reward System formula from doc" | "What's the behavioral loop from user's perspective?" |
| **L3** | Skipping failure modes | Only describes happy paths | "What happens with abandonment? Missing data?" |
| **L4** | Actual values vs placeholders | "Tasks earn 100 points" | "Use placeholder like `[base_task_points]`" |
| **L4** | Referencing user stories | "As per user story #47..." | "What's the system rule independent of stories?" |
| **L4** | Writing code vs conceptual | Provides code snippets | "Describe the logic conceptually, not as code" |
| **L5** | Implementation in user stories | "TaskManager class updates database" | "Focus on user intent: 'save task, don't lose work'" |
| **L5** | Vague acceptance criteria | "It should work properly" | "What specific behaviors would confirm it's working?" |
| **L5** | Values without rationale | "Timeout = 30 seconds" | "Why 30 seconds? What's the reasoning?" |

---

### Validation Checklists

Use this matrix to verify each level is complete before descending:

| Validation Criterion | L1 | L2 | L3 | L4 | L5 |
|---------------------|----|----|----|----|-----|
| **Core Content Complete** | Mission & inviolable principles articulated | Design philosophies with justification | Complete user journeys documented | All subsystems with clear boundaries | Stories organized by priority/journey/system |
| **Proper Detail Level** | Emotional goals specific (not "feel good") | Strategy explained, not implementation | Behavioral flow, not UI details | Rules conceptual, not code | Observable acceptance criteria |
| **Reference Constraints** | No project docs referenced | No L3-5 referenced | No L4-5 referenced | No L5 referenced | Can reference all levels |
| **Numeric Values** | No numbers (except philosophical) | Placeholders only | Placeholders only | Placeholders only | Actual values with rationale |
| **Completeness Checks** | Anti-goals defined; North-star described | Trade-offs documented; Success criteria align with L1 | Failure modes explored; System coordination described | Data models defined; Dependencies mapped | Stories validate experience; Tuning approach documented |
| **Special Requirements** | No features/systems/implementation | External frameworks referenced if applicable | Verification criteria for implementers | State transitions specified | L5 Stories: As [user], I want [action], so [outcome] format |
| **Special Requirements** | ‚Äî | ‚Äî | ‚Äî | ‚Äî | L5 Fine Tuning: Parameters organized logically; References to source systems |
| **Market Context (Optional)** | If present: Competitors/Table Stakes/Pain Points/Influences before Mission; lightweight format; external references only | ‚Äî | ‚Äî | ‚Äî | ‚Äî |

---

### Cross-Cutting Concerns Guide

Some themes span multiple levels. Here's where each aspect belongs:

| Concern | Level 1 (WHY) | Level 2 (Strategy) | Level 3 (Interaction) | Level 4 (Systems) | Level 5 (Implementation) |
|---------|---------------|--------------------|-----------------------|-------------------|--------------------------|
| **Accessibility** | Core principle ("All deserve equal access") | WCAG compliance, keyboard-first | Navigation patterns, feedback modalities | ARIA attributes, focus management | User story validation + timing |
| **Security & Privacy** | Inviolable principle ("User data is sacred") | Zero-trust, encryption-at-rest | Auth/authz interaction flows | Permission models, data isolation | Security stories + config parameters |
| **Internationalization** | Global accessibility mission | Translation approach, cultural adaptation | RTL support, date formats | Locale storage, translation keys | i18n stories + locale tuning |
| **Performance** | UX principle ("Respect users' time") | Perceived vs. actual speed, progressive enhancement | Optimistic updates, loading states | Caching strategies, lazy loading | Performance stories + timing parameters |

**Key principle:** The concern's **philosophy** lives at high levels, while **implementation details** cascade down to appropriate levels.

---

### Example Dialog Patterns

These examples show effective AI-user interactions during spec creation.

#### **Example 1: Extracting True "Why" (Level 1)**

**Problem:** User jumps to features ("task list and point system") instead of defining purpose.

**Solution progression:**
1. Challenge surface statement ("People need to get more done")
2. Identify gap in existing solutions ("They make people feel burned out")
3. Extract emotional goal ("Energized and in control, not drained")
4. Crystallize principle ("Sustainable productivity through energy awareness")
5. Define inviolable constraint ("Never add features that make users feel guilty")

**Key lesson:** Keep asking "why" until you reach emotional/philosophical foundation.

---

#### **Example 2: Defining Interaction Architecture (Level 3)**

**Problem:** User describes UI ("green checkmark appears") instead of behavioral pattern.

**Solution - Map complete interaction loop:**
- **Trigger:** User decides task is finished
- **Action:** Tap/click to mark complete
- **Reward:** Visual confirmation + points increase
- **Investment:** Points accumulate toward future rewards

**Result:** Hook Model loop documented at conceptual level, independent of visual design.

**Key lesson:** Guide users to describe behavioral loops, not visual implementations.

---

#### **Example 3: Redirecting Level Jumps**

**Problem:** User places system rule ("tasks earn points based on difficulty") in Level 1.

**Solution - Separate philosophy from implementation:**
- **Level 1 (Philosophy):** "Value meaningful effort over mere completion"
- **Level 4 (System Rule):** Points-per-difficulty calculation

**Key lesson:** Continuously enforce hierarchy; redirect implementation details to appropriate levels.

---

## Benefits

This framework provides:

- **Philosophical stability** ‚Äî Core values don't change with implementation details
- **Implementation flexibility** ‚Äî Completely refactor systems without changing principles
- **Clear decision boundaries** ‚Äî Know where each type of decision belongs
- **Reduced cognitive load** ‚Äî Each document has a clear scope and audience
- **Easier onboarding** ‚Äî New team members can read from Level 1 downward
- **Safer AI assistance** ‚Äî AI systems can understand and respect architectural constraints
- **Testable integrity** ‚Äî Can automatically validate reference rules
